{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Подготовка признак-векторов и меток",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew-veriga/Tensorflow-labs/blob/master/PrepareFeaturesLabels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmLEmOfj2kXW"
      },
      "source": [
        "###Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tS-TGjQ2olm"
      },
      "source": [
        "подключаем знакомые нам `tensorflow`, `numpy` и библиотеку рисования графиков "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOjujz601HcS"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPW9_GQz3C9-"
      },
      "source": [
        "Мы используем здесь класс `tf.data.Dataset`, и чтобы создать себе некоторые данные, сделаем диапазон из 10 последовательных чисел. Будет удобно контролировать правильность наших действий, учитывая, что эти ряды должны равномерно возрастать. В реальных рядах такого удобства не будет, поэтому лучше заранее всё отладить на понятных числах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asEdslR_05O_"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "for val in dataset:\n",
        "   print(val.numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvX8_7cG4HAN"
      },
      "source": [
        "Итак, теперь сделаем это немного интереснее. Мы будем использовать `dataset.window`, чтобы расширить наш набор данных с помощью сдвигающегося окна. Первый параметр `size` указывает размер окна, параметр `stride` указывает шаг, с которым отбираются значения, а `shift` - величину сдвига окна. Чтобы хорошо понять, как всё это работает, поэкспериментируйте с этими параметрами функции `window` поменяйте их в разных сочетаниях и посмотрите, что получится.\\\n",
        "*один фрагмент, выделенный окном в этом примере, выводится как горизонтальный ряд чисел, весь полученный набор должен выглядеть как стопка горизонтальных массивов*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrv_ghSt1lgQ"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, stride=1,shift=1)\n",
        "for window_dataset in dataset:\n",
        "  for val in window_dataset:\n",
        "    print(val.numpy(), end=\" \")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpVisGIo6KHi"
      },
      "source": [
        "Мы получили выходные данные первых пяти элементов, а затем вторых пяти элементов, а затем третьих пяти элементов и т. д. В конце набора данных, когда данных недостаточно, чтобы дать нам пять элементов, вы увидите более короткие строки. Но это не то, что нужно, ведь все признак-векторы должны быть одинаковой длины.\\\n",
        "Чтобы отбросить фрагменты с недостаточным количеством значений, мы установим параметр `drop_reminder` в значение true.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLEq6MG-2DN2"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
        "for window_dataset in dataset:\n",
        "  for val in window_dataset:\n",
        "    print(val.numpy(), end=\" \")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiXusHSEE8pZ"
      },
      "source": [
        "Теперь давайте поместим эти данные в списки, чтобы можно было начать использовать их в машинном обучении.\\\n",
        "`TensorFlow` любит, чтобы его данные были в формате `numpy`.\\\n",
        " Ну, мы можем легко преобразовать его, вызвав метод `.numpy()`, и когда распечатаем его, увидим, что он теперь указан в квадратных скобках."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ9CAHlJ2ODe"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda w: w.batch(5))\n",
        "for w in dataset:\n",
        "  print(w.numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj-fZc1zNKzW"
      },
      "source": [
        "Хорошо, теперь нам надо разделить данные на признак-векторы и метки. В каждом элементе надо для признак-вектора оставить все значения, кроме последнего, которое будет меткой. И это может быть достигнуто с помощью функции `map()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DryEZ2Mz2nNV"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
        "# мы разбиваем каждый массив на две части: \n",
        "# первая содержит все элементы кроме последнего: [:-1], а вторая содержит только последний элемент: [-1:]\n",
        "dataset = dataset.map(lambda window: (window[:-1], window[-1:]))\n",
        "for x,y in dataset:\n",
        "  print(x.numpy(), y.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIcAJn3EO2Wy"
      },
      "source": [
        "и теперь наш набор данных выглядит как хороший набор признаков и меток.\\\n",
        "И осталось только перемешать его. Это поможет нам избежать предвзятости в  последовательности. \\\n",
        "И это возможно с помощью метода `.shuffle()`. Мы вызовем его с параметром `buffer_size=10`, потому что это то максимальное количество строк, которые у нас могут быть.\\\n",
        "Многократные прогоны покажут строки в разной последовательности, потому что они перемешиваются случайным образом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tl-0BOKkEtk"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda w: w.batch(5))\n",
        "dataset = dataset.map(lambda w: (w[:-1], w[-1:]))\n",
        "dataset = dataset.shuffle(buffer_size=10)\n",
        "for x,y in dataset:\n",
        "  print(x.numpy(), y.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKgN1vwf3L1a"
      },
      "source": [
        "*предвзятость (bias) последовательности - это когда порядок может влиять на выбор. Например, если бы я спросил вас, какой ваш любимый сериал и перечислил в таком порядке:«Игра престолов», «Убийство Евы», «Путешественники» и «Доктор Кто» , вы, скорее всего, выберете «Игра Престолов», так он вам знаком и он стоит первым. Даже если вам одинаково нравятся и другие сериалы. При обучении данных в наборе данных мы не хотим, чтобы последовательность влияла на обучение аналогичным образом, поэтому перемешать их бывает полезно.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WINOHQFKSVPE"
      },
      "source": [
        "Наконец, пакетирование данных. и это делается с помощью метода `.batch()'.\\\n",
        "Устанавливая размер пакета в 2, мы объединим наши данные  по два x и два y вместе. Ранее мы не пользовались таким разбиением данных на пакеты, keras все делал за нас, если это было нужно. Но теперь попробуем другой подход"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa0PNwxMGapy"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10)\n",
        "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda w: w.batch(5))\n",
        "dataset = dataset.map(lambda w: (w[:-1], w[-1:]))\n",
        "dataset = dataset.shuffle(buffer_size=10)\n",
        "dataset = dataset.batch(2).prefetch(1)\n",
        "for x,y in dataset:\n",
        "  print(\"x = \", x.numpy())\n",
        "  print(\"y = \", y.numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbiyF4gaUn2I"
      },
      "source": [
        "Итак, это ноутбук с кодом, который разбивает временной ряд данных скользящим окном на размеченные признак-векторы. Попробуйте поиграть с параметрами в последней ячейке в тех методах, которые вам показались непонятными и как только вы хорошо разберетесь с тем, что тут происходит, переходите к следующей главе.\n",
        "Там вы увидите сезонный набор данных, и узнаете, как с помощью техники скользящего окна получить из него нужные x и y для нейронной сети, чтобы обучить ее прогнозированию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezS6PRYQVqE3"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}