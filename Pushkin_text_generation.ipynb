{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "text_generation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrew-veriga/Tensorflow-labs/blob/master/Pushkin_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "Это руководство скопировано с авторского с заменой обучающего набора данных на русский. В качестве датасета здесь используются стихи Пушкина.\n",
        "\n",
        "Оригинальный Jupiter Notebook с примерами на шекспировских текстах:\n",
        "https://www.tensorflow.org/tutorials/text/text_generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GCCk8_dHpuNf"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Генерация текста с RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "В этом уроке показано, как создавать текст с помощью RNN на основе символов. Мы будем работать с набором данных пушкинских стихов по статье [«Необоснованная эффективность рекуррентных нейронных сетей»](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) Андрея Карпати.\n",
        "\n",
        " На основании последовательности символов из этих данных ('Пушки'), обучить модель предсказывать следующий символ в последовательности ('н'). Продолжение последовательности текста может быть сгенерировано повторным вызовом модели с входом, включающим последнюю предсказанную букву.\n",
        "\n",
        "Примечание: Включите ускорение графического процессора, чтобы выполнить этот ноутбук быстрее.\n",
        "In Colab: *Runtime > Change runtime type > Hardware acclerator > GPU*.\n",
        "\n",
        "Этот руководство включает в себя запускаемый код, реализованный с помощью [tf.keras](https://www.tensorflow.org/programmers_guide/keras) и [eager execution](https://www.tensorflow.org/programmers_guide/eager). \n",
        "Ниже приводится образец вывода, когда модель в этом учебнике была тренирована в течение 100 эпох, и начала со строки 'И вот':\n",
        "\n",
        "```\n",
        "И вот мудрецов,\n",
        "Глазами ученик,\n",
        "И недоветельных и строгих),\n",
        "Ученый малый, но Евгений\n",
        "Наедине с своей душой\n",
        "Был недоволен сам собой.\n",
        "И поделом: в разборе строгом,\n",
        "На тайники друг свободу заицуда\n",
        "И шум немирных челноков.\n",
        "Я вдаль уплыл, надежды полны;\n",
        "```\n",
        "\n",
        "Хотя некоторые предложения - грамматические, большинство из них не имеет смысла. Модель не понимает значения слов, но обратите внимание:\n",
        "\n",
        "- Модель основана на последовательностях букв. Когда обучение началось, модель не знала, ни как пишется слово, ни даже то, что слово - это единица текста.\n",
        "\n",
        "- Как показано ниже, модель обучается на небольших пакетах текста (по 100 символов каждый) и тем не менее способна генерировать более длинную последовательность текста с согласованной структурой.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Импорт TensorFlow and необходимых библиотек"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n--zQC0WoKBC"
      },
      "source": [
        "###Загрузка текста из Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wfwWEw_oxFR"
      },
      "source": [
        "Чтобы файл был доступен из вашего диска, можно предоставить на него доступ на просмотр \"для всех\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHWsJf-A1o6J"
      },
      "source": [
        "# Импорт PyDrive и связанных с ним библиотек.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "#https://drive.google.com/file/d/1pLbtXYa7kYSOoqgv3U_FPYyAvOrH0l0B/view?usp=sharing\n",
        "# Download a file based on its file ID.\n",
        "file_id = '1pLbtXYa7kYSOoqgv3U_FPYyAvOrH0l0B'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "#print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavnuByVymwK"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = downloaded.GetContentString()\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Duhg9NrUymwO"
      },
      "source": [
        "# Take a look at the first 200 characters in text\n",
        "print(text[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlCgQBRVymwR"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Обработка текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Векторизация текста\n",
        "Перед тренировкой нужно представить строки в виде чисел. Созадим две таблицы: одна для поиска числа по символу, другая - символа по числу.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IalZLbvOzf-F"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Теперь у нас есть числовое представление для каждого символа. Обратите внимание, что мы отображаем символ как индекс от 0 до\n",
        "`len(unique)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYyNlCNXymwY"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1VKcQHcymwb"
      },
      "source": [
        "# Посмотрим, как первые 13 символов из текста отображаются в челые числа\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### Прогнозирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Задача, которой мы обучаем модель: **по заданному символу, или последовательности символов, выбрать  следующий символ.**\n",
        "\n",
        "Поскольку RNN сохраняет внутреннее состояние, зависящее от ранее виденных элементов, то зная все символы, вычисленные до этого момента, она может предсказать следующий  символ на каждом шаге времени.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Создание обучаемых примеров и меток\n",
        "Теперь делим текст на примеры последовательностей. Каждая входящая последовательность будет содержать `seq_length` символов из текста.\n",
        "\n",
        "Для каждой входящей последовательности соответствующие метки содержат такую же длину текста, но  смещены на один символ вправо.\n",
        "\n",
        "Итак разобьем текст на куски `seq_length` + 1. Например, скажем, `seq_length=4`, и наш текст 'рыбка'. Входная последовательность будет 'рыбк', и целевая последовательность - 'ыбка'.\n",
        "\n",
        "Для этого сначала используйте функцию 'tf.data.Dataset.from_tensor_slices' для преобразования вектора текста в поток индексов символов.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UHJDA39zf-O"
      },
      "source": [
        "# Максимальная длина предложения для единичного ввода символов\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Создаем обучающие примеры и метки\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "Метод `batch` позволяет легко преобразовывать эти отдельные символы в последовательности требуемого размера."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4hkDU3i7ozi"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "Каждую последовательность дублировать и сдвинуть для создания ввода и метки. Применим к каждому пакету метод `map` с простой функцией сдвига:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiCopyGZymwi"
      },
      "source": [
        "Напечатаем первый пример и метку:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNbw-iR0ymwj"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_33OHL3b84i0"
      },
      "source": [
        "Каждый индекс этих векторов обрабатывается как один шаг времени. Для ввода в шаге времени 0 модель получает индекс для первого символа 'Н' и пытается предсказать индекс для 'е' в качестве следующего символа. В следующий шаг времени она делает то же самое, но RNN рассматривает предыдущий контекст шага в дополнение к текущему символу ввода.\n",
        "\n",
        "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the `RNN` considers the previous step context in addition to the current input character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eBu9WZG84i0"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Создание тренировочных пакетов\n",
        "\n",
        "Мы использовали `tf.data`, чтобы разделить текст на управляемые последовательности. Но прежде чем залить эти данные в модель, нам нужно перетасовать данные и упаковать их в пакеты."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pGotuNzf-S"
      },
      "source": [
        "# Размер пакета\n",
        "BATCH_SIZE = 64\n",
        "# Размер буфера для перетасовки набора данных\n",
        "# (данные TF предназначены для работы с возможными бесконечными последовательностями,\n",
        "# Так что он не пытается перетасовать всю последовательность в памяти. Вместо этого\n",
        "# он поддерживает буфер, в котором перемешивает элементы).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Строим модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "Для определения модели используйте tf.keras.Sequential. В этом простом примере в модели используются три слоя:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: Входиной слой. Таблица обучаемых поисков, которая свяжет  номер каждого символа с вектором размерностью `embedding_dim`;\n",
        "\n",
        "* `tf.keras.layers.GRU`: Тип RNN с размером `units=rnn_units` (Тут также можно использовать слой LSTM.)\n",
        "\n",
        "* `tf.keras.layers.Dense`: выходной слой с количеством юнитов `vocab_size` \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "# размер словаря\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# размерность векторов встраивания\n",
        "embedding_dim = 256\n",
        "\n",
        "# количество юнитов RNN \n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtCrdfzEI2N0"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwsrpOik5zhv"
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "Для каждого символа модель смотрит на эмбеддинг-вектор, запускает GRU на один шаг времени со эмбеддингом в качестве входа и выводит в полносвязный слой для генерации логистических юнитов, предсказывающих максимальное правдоподобие входа следующего символа:\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/andrew-veriga/Tensorflow-labs/blob/master/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Пробуем модель\n",
        "Теперь запустите модель, чтобы увидеть, что она ведет себя как ожидалось.\n",
        "\n",
        "Сначала проверьте форму вывода:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_70kKAPrPU"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "В приведенном выше примере длина ввода последовательности равна `100`, но модель может быть запущена на входных данных любой длины:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPGmAAXmVLGC"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "Чтобы получить от модели предсказания, нам нужно выбрать из распределения на выходе актуальные индексы. Это распределение определяется логитами по словарю символов.\n",
        "\n",
        "Примечание: Важно, что *выборка* из распределения, взятая просто как *argmax* распределения может легко привести модель к зацикливанию.\n",
        "\n",
        "Попробуйте это для первого примера в пакете:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "Это дает нам на каждом шаге во времени предсказание следующего индекса символов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqFMUQc_UFgM"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "декодируем их, чтобы посмотреть текст, предсказанный этой нетренированной моделью:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWcFwPwLSo05"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Тренировка модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "На этом этапе проблема может рассматриваться как стандартная задача классификации: По предыдущему состоянию RNN и входу на текущий шаг времени предсказать класс следующего символа."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### добавляем оптимизатор и функцию потерь"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "Стандартная функция потерь `tf.keras.losses.sparse_categorical_crossentropy` работает в этом случае, поскольку она применяется на самой последней размерности прогнозов.\n",
        "\n",
        "Поскольку наша модель возвращает логиты, нам необходимо установить флаг `from_logits`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HrXTACTdzY-"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Компилируем процедуру обучения с использованием метода `tf.keras.Model.compile`\n",
        "Используем `tf.keras.optimizers.Adam` с аргументами по умолчанию и нашу функцию потерь."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Настройка контрольных точек"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Используем `tf.keras.callbacks.ModelCheckpoint` чтобы обеспечить сохранение контрольных точек во время обучения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "source": [
        "# Каталог для сохранения контрольных точек\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Имена файлов контрольных точек\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Тренируем модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UK-hmKjYVoll"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "##Генерация текста\n",
        "### Восстановим последнюю контрольную точку"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyeYRiuVjodY"
      },
      "source": [
        "для простоты, на этом шаге прогнозирования используем размер пакета 1.\n",
        "\n",
        "Способ, которым состояние RNN передается от одного временного шага к другому, позволяет модели получать фиксированный размер пакета после построения.\n",
        "\n",
        "Чтобы запустить модель с другим batch_size, нам нужно перестроить модель и восстановить веса с контрольной точки.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk2WJ2-XjkGz"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LycQ-ot_jjyu"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71xa6jnYVrAN"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### Цикл генерации\n",
        "Следующий блок кода генерирует текст:\n",
        "\n",
        "* начинается с выбора стартовой строки, инициализации состояния RNN и задания количества символов для генерации.\n",
        "\n",
        "* Получаем распределение предсказания следующего символа с помощью стартовой строки и состояния RNN.\n",
        "\n",
        "* используем распределение по категориям для расчета индекса прогнозируемого символа. Этот прогнозируемый символ используется в качестве следующего входа в модель.\n",
        "\n",
        "Состояние RNN, возвращенное моделью, снова отправяется в модель, так что теперь оно имеет больше контекста, а не только одно слово. После предсказания следующего слова, измененные состояния RNN снова подается обратно в модель, которая, пока обучается,  получает все больше контекста из ранее предсказанных слов.\n",
        "\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/andrew-veriga/Tensorflow-labs/blob/master/text_generation_sampling.png?raw=1)\n",
        "\n",
        "Посмотрев на сгенерированный текст, вы увидите, что модель знает, когда ставить большие буквы, делать абзацы и имитирует Пушкинский стиль. С небольшим количеством эпох обучения она еще не научилась формировать последовательные предложения.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktovv0RFhrkn"
      },
      "source": [
        "print(generate_text(model, start_string=u\"и вот\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "Самое простое, что вы можете сделать для улучшения результатов, это тренировать его дольше (попробуйте `EPOCHS=30`).\n",
        "\n",
        "Вы также можете поэкспериментировать с другой начальной строкой или попробовать добавить еще один слой RNN, чтобы повысить точность модели, или настроить параметр температуры, чтобы генерировать более или менее случайные прогнозы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## Дополнение: индивидуальная настройка обучения\n",
        "\n",
        "Описанная выше процедура обучения проста, но не дает вам особого контроля.\n",
        "\n",
        "Итак, теперь, когда вы увидели, как запустить модель вручную, давайте распакуем цикл обучения и реализуем его самостоятельно. Это дает отправную точку, например, для реализации _curriculum learning_, чтобы помочь стабилизировать выходные данные модели без обратной связи.\n",
        "\n",
        "Будем использовать `tf.GradientTape` для отслеживания градиентов. Вы можете почитать об этом здесь: [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "Процедура работает следующим образом:\n",
        "\n",
        "* Во-первых, инициализируйте состояние RNN. Мы делаем это, вызывая метод `tf.keras.Model.reset_states`.\n",
        "\n",
        "* Затем выполняется итерация по датасету (одна итерация для пакета) и вычисляется *прогноз* для пакета.\n",
        "\n",
        "* Открывается объект `tf.GradientTape` и рассчитывается потеря в этом контексте.\n",
        "\n",
        "* Вычисляются градиенты потерь по переменным модели, с использованием метода `tf.GradientTape.grads`.\n",
        "\n",
        "* Наконец, следующий шаг обновления весов по вычисленным градиентам, с использованием метода оптимизатора `tf.train.Optimizer.apply_gradients`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XAm7eCoKULT"
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUKhnZtMVpoJ"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4kH1o0leVIp"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4tSNwymzf-q"
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otH1ViT_Qnbg"
      },
      "source": [
        "checkpoint_prefix.format(epoch=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}